---
# Please keep this file alphabetically sorted!
commons:
  cask: CASK
  cdap: CDAP
  entity:
    application:
      plural: Applications
      short-singular: App
      short-plural: Apps
      singular: Application
    artifact:
      plural: Artifacts
      singular: Artifact
    cdap-data-pipeline:
      plural: Data Pipelines
      singular: Data Pipeline
    cdap-data-streams:
      plural: Data Streams
      singular: Data Stream
    dataset:
      plural: Datasets
      singular: Dataset
    datasetinstance:
      plural: Datasets
      singular: Dataset
    flow:
      plural: Flows
      singular: Flow
    mapreduce:
      plural: Mapreduce
      singular: Mapreduce
    program:
      plural: Programs
      singular: Program
    service:
      plural: Services
      singular: Service
    spark:
      plural: Spark
      singular: Spark
    stream:
      plural: Streams
      singular: Stream
    worker:
      plural: Workers
      singular: Worker
    workflow:
      plural: Workflows
      singular: Workflow
    view:
      plural: Stream Views
      singular: Stream View
  hydrator: Cask Hydrator
  keyValPairs:
    keyPlaceholder: key
    valuePlaceholder: value
  market: Cask Market
  resource-center: Resource Center
  tracker: Cask Tracker
  nameLabel: Name
  descriptionLabel: Description
  formatLabel: Format
  schemaLabel: Schema
features:
  ConfirmationModal:
    confirmDefaultText: OK
    cancelDefaultText: Cancel
  Home:
    Title: CDAP - Home
    emptyMessage: No entities found in the system.
    Cards:
      type: "Type: "
    Header:
      filters: Filters
      search-placeholder: Search entities ...
      sort: Sort
      pagination: Page
      sortOptions:
        nameAsc:
          displayName: A - Z
        nameDesc:
          displayName: Z - A
  Dashboard:
    Title: Dashboard

  FastAction:
    deleteConfirmation: Are you sure you want to delete *_{entityId}_*?
    deleteLabel: Delete
    deleteFailed: Failed to delete {entityId}
    truncateConfirmation: Are you sure you want to truncate *_{entityId}_*?
    truncateLabel: Truncate
    truncateFailed: Failed to truncate {entityId}

  JumpButton:
    buttonLabel: Jump
    viewHydrator: View in Hydrator
    viewTracker: View in Tracker

  Management:
    Title: Management
    Top:
      version-label: Version
      time-label: Uptime
      services: Services
      updated: Last updated
      updated-label:
        plural: seconds ago
        singular: second ago
    Panels:
      nodes: Nodes
      virtual-cores: Virtual Cores
      memory: Memory
      application: Application
    Services:
      appfabric: App Fabric
      log_saver: Log Saver
      metrics_processor: Metrics Processor
      dataset_executor: Dataset Executor
      metadata_service: Metadata Service
      streams: Streams
      explore_service: Explore Service
      metrics: Metrics
      transaction: Transaction
    Configure:
      title: Configure
      buttons:
        add-ns: Add Namespace
        view-config: View Configurations
        manage-ns: Manage Namespaces
        delete-ns: Delete Namespace
        manage-roles: Manage Roles
        reset-instance: Reset Instance
        tag-management: Tag Management
        instance-preference: Instance Preference
        delete-datasets: Delete All Datasets
        view-invalid: View Invalid Transactions
    DetailPanel:
      headers:
        nodes: Nodes
        entities: Entities
        apps: Apps
        blocks: Blocks
        storage: Storage
        queues: Queues
        vcores: VCores
        memory: Memory
      labels:
        accepted: Accepted
        available: Available
        corrupt: Corrupt
        currentCapacity: Current Capacity
        deadRegionServers: Dead Region Servers
        decommissioned: Decomissioned
        decommissionInProgress: Decomission In Progress
        failed: Failed
        healthy: Healthy
        killed: Killed
        liveRegionServers: Live Region Servers
        lost: Lost
        maxCapacity: Max Capacity
        new: New
        new_saving: New Saving
        masters: Masters
        missing: Missing
        namespaces: Namespaces
        running: Running
        rebooted: Rebooted
        stopped: Stopped
        submitted: Submitted
        tables: Tables
        total: Total
        totalRegionServers: Total Region Servers
        underreplicated: Underreplicated
        used: Used
        unhealthy: Unhealthy

    Component-Overview:
      label: Component Overview
      headers:
        cdh: CDH
        yarn: YARN
        hbase: HBASE
        hdfs: HDFS
        zookeeper: Zookeeper
        kafka: Kafka
        spark: Spark

  Market:
    search-placeholder: Search
    connectErrorMessage: Cannot connect to Market
    tabs:
      all: All
      applications: Applications
      artifacts: Artifacts
      dashboards: Dashboards
      datapacks: Datapacks
      datasets: Datasets
      examples: Examples
      pipelines: Pipelines
      plugins: Plugins
      useCases: Use Cases
    action-types:
      create_stream:
        name: Create
      create_app:
        name: Create
      create_pipeline_draft:
        name: Create
      create_pipeline:
        name: Create
      create_artifact:
        name: Create
      informational:
        name: Download
      load_datapack:
        name: Load
  MarketPlaceEntity:
    Metadata:
      version: Version
      author: Author
      company: Company
      created: Created
      cdapversion: CDAP Version
  MarketEntityModal:
    version: "Version :"
  Navbar:
    CDAP:
      dashboard: Dashboard
      home: Home
      management: Management
    HeaderActions:
      caskHome: Cask Home
      documentation: Documentation
      logout: Logout
      signedInAs: Signed in as
      support: Support
    Sidebar:
      extension: "Extensions:"

  Resource-Center:
    Stream:
      label: Stream
      description: Stream is used to ingest data into HDFS in real-time or batch.
      actionbtn0: Create
    Application:
      label: Application
      description: Upload a custom Application artifact or create an instance of Application from existing Artifact.
      actionbtn0: Upload
      actionbtn-1: Create
    Stream-View:
      label: Stream View
      description: Stream View is non-materialized view over a Stream with schema on read capability. Create a view over existing Stream.
      actionbtn0: Create
    HydratorPipeline:
      label: Hydrator Pipeline
      description: Hydrator pipeline allows you to ingest, egress and process data to, from and within Hadoop.
      actionbtn0: Create
    Artifact:
      label: Artifact
      description: Artifact is a JAR file that contains either a native Application or collection of Plugins.
      actionbtn0: Upload
    Plugins:
      label: Plugins
      description: Plugin is a easy way to extend the functionality of a Application.
      actionbtn0: Upload

  SplashScreen:
    buttons:
      getStarted: Read the Docs
      introduction: Intro to CDAP
      register: Register for Updates
    intro-message: Unified Integration Platform for Big Data
    title: Welcome to Cask Data Application Platform
    version-label: Version 4.0.0 Preview

  SpotlightSearch:
    noResult: No results found
    showAll: Show all {num} results
    SpotlightModal:
      headerSearchResults: "Search results for: {query}"
      numResults: "{total} results found"

  Wizard:
    ArtifactUpload:
      Step1:
        description: Upload your artifact JAR
        shorttitle: Upload Artifact
        title: Upload JAR
        uploadHelperText: Upload the third party artifact that was downloaded from the previous step
        filePathLabel: Choose file
      Step2:
        description: Configure the settings for your artifact
        shorttitle: Artifact Configuration
        title: Configure Artifact
        parentArtifactLabel: Parent Artifact
        nameLabel: Name
        typeLabel: Type
        classnameLabel: Class Name
        descriptionLabel: Description
      headerlabel: Upload Artifact
    Done: Done
    Informational:
      Step1:
        description: Please follow the steps specified below to configure Hydrator plugin to read from your database
        shorttitle: Third Party Artifact Information
        title: Information
      headerlabel: Third Party Artifact
    Skip: Skipped
    FailedMessage: Failed to {step}
    StreamCreate:
      Step1:
        shorttitle: General Information
        title: General
        description: Provide information about Stream you want to create.
        ttllabel: Event Life Time
        ttl-placeholder: Specify time to live events in seconds
      Step2:
        shorttitle: Setup Format and Schema
        title: Set Format and Schema
        description: Setting format and schema allows you to perform schema-on-read.
      Step3:
        shorttitle: Trigger Setup
        title: Setup Trigger
        description: Setting up Trigger configures system to notify systems observing to start processing.
        thresholdlabel: The stream will notify any observers upon reaching this threshold to start processing of the data in this Stream
        mblabel: Megabytes (MB)
      Step4:
        shorttitle: Upload Data
        title: Upload Data
        description: Upload Data to the Stream you created
      headerlabel: Create Stream
    UploadData:
      Step1:
        shorttitle: View Data
        title: View Data
        description: Shows the data for your reference that would be uploaded to a destination
      Step2:
        shorttitle: Select Destination
        title: Select Destination
        description: Select the destination where the data needs to be uploaded
        dataentitynameplaceholder: Dataset/Stream Name
        destinationtype: Destination Type
        destinationname: Destination Name
      headerlabel: Upload Data
    PublishPipeline:
      Step1:
        shorttitle: Configure pipeline
        title: Configure pipeline
        description: Specify the name of the pipeline
      pipelinenameplaceholder: Pipeline name
      headerlabel: Publish Pipeline
    HydratorPipeline:
      title: Create Data Pipeline
      message: Choose the pipeline type you would like to create
      batchLinkLabel: Batch Pipeline
      realtimeLinkLabel: Realtime Pipeline

    Add-Namespace:
       Step1:
         ssd-label: "General Information"
         sld-desc:  "Specifies the name and the description of Namespace"
         name-label: "Name"
         name-placeholder: "Namespace Name"
         description-label: "Description"
         description-placeholder: "Description"
         scheduler-queue-label: "Scheduler Queue"
         scheduler-queue-placeholder: "Specify the YARN queue name to be associated with this namespace"
       Step2:
         ssd-label: "Namespace Mapping"
         sld-label: "Specifies mapping of namespace resources to the underlying existing storage resources"
         hdfs-root-directory-label: "HDFS Root Directory"
         hdfs-root-directory-placeholder: "Root directory of HDFS for new namespace root"
         hive-db-name-label: "Hive Database Name"
         hive-db-name-placeholder: "Hive db name for new namespace"
         hbase-nm-name-label: "HBase Namespace Name"
         hbase-nm-name-placeholder: "Existing namespace in HBase for new namespace"
       Step3:
         ssd-label: "Security"
         sld-label: "Specifies credentials for securely impersonating programs running as specified user"
         principal-label: "Principal"
         principal-placeholder: "User to be impersonated for jobs in new namespace"
         keytab-uri-label: "Keytab URI"
         keytab-uri-placeholder: "Location of keytab file associated with the principal"
       Step4:
         ssd-label: "Preferences"
         sld-label: "Specify preferences to applied at the namespace level"
         name-label: "Name"
         name-placeholder: "Preference name"
         value-label: "Value"
         value-placeholder: "Preference value"
       headerlabel: Add Namespace
       Status:
         creation-error-desc: "Failed to create namespace '%s'"
         creation-success-desc: "Successfully created namespace '%s'"

...
